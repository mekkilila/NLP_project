{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import keras\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import re\n",
    "import random\n",
    "from gensim.models import Word2Vec\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> Pre training word vectors (embeddings) using unlabeled data = unsupervised manner.\n",
    "Model : similar to Word2Vec to learn distributed representations of words (allow to capture semantic relationships between words and synonyms in similar contexts). \n",
    "-> Supervised learning using labeled data : word embeddings previously learnt as input features for training sentiment classifier, which was a logistic regression (permits the classifier to learn word embedding to map reviews to sentiment labels). Step for fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectif de l'article : \n",
    " Apprendre des word vectors non pas via des méthodes non supervisées classiques (comme Word2Vec), mais en les supervisant directement via une tâche de classification de sentiment.\n",
    "\n",
    " Mise en place modèle :\n",
    " - 1 : Tenter approche non supervisée classique Word2Vec classique et observer résultats (moyens?)\n",
    " - 2 : Approche de l'article \n",
    " - 3 : Trouver d'autres méthodes état de l'art et tester pour comparer ? \n",
    "\n",
    " Dataset : \n",
    " - phrases et sous-phrases extraites de critiques de film (Rotten Tomatoes)\n",
    " - chaque sous-phrase annotée avec score sentiment (fine-grained ou binaire)\n",
    " \n",
    "\n",
    " Approche principale : \n",
    " Utilisation modèle récursif (RNN) structuré selon grammaire phrases\n",
    " Chaque mot représenté par vecteur\n",
    " Vecteurs combinés récursivement selon structure syntaxique (parse tree) pour produire représentation de phrase.\n",
    " Supervision à chaque noeud de l'arbre = permet ajuster vecteurs de mot selon contribution au sentiment \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A répliquer : \n",
    "- Prétraitement : tokenisation, parsing syntaxique (parser de constituants type Stanford Parser), extraction de toutes sous phrases (phrases, clauses, etc.)\n",
    "- Modèle : Implémenter RNN sur arbres syntaxiques (matrice embedding, fonction de composition f(W[v1;v2] +b), classificateur au dessus représnetations des noeuds pour prédire sentiment)\n",
    "- Entraînement : loss supervisée à chaque noeud (cross-entropy), propagation gradient à travers recherche recursive "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "implémentation : https://github.com/stanfordnlp/treelstm\n",
    "\n",
    "Comparer la perf avec approches modernes : LSTM, BERT, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Stanford Sentiment Treebank is a corpus with fully labeled parse trees that allows for a complete analysis of the compositional effects of sentiment in language. The corpus is based on the dataset introduced by Pang and Lee (2005) and consists of 11,855 single sentences extracted from movie reviews. It was parsed with the Stanford parser and includes a total of 215,154 unique phrases from those parse trees, each annotated by 3 human judges.\n",
    "\n",
    "Each phrase is labelled as either negative, somewhat negative, neutral, somewhat positive or positive. The corpus with all 5 labels is referred to as SST-5 or SST fine-grained. Binary classification experiments on full sentences (negative or somewhat negative vs somewhat positive or positive with neutral sentences discarded) refer to the dataset as SST-2 or SST binary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lMD \n",
    "- 25,000 labeled reviews for training\n",
    "(Highly polar — i.e., very positive or very negative)\n",
    "- 25,000 labeled reviews for testing\n",
    "- 50,000 unlabeled reviews for unsupervised pre-training\n",
    "- Raw text and bag-of-words format included\n",
    "- Binary classification only (positive vs. negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb  # preprocessed version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "sst_dataset = load_dataset(\"sst\", \"default\")  # pour fine-grained\n",
    "# Ou : load_dataset(\"sst2\") pour version binaire\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Unsupervised part**\n",
    "\n",
    "I use the unsupervised part of the train set in order to learn word embeddings. I will compare the results of known embeddings such as Word2Vec, GloVe, and FastText, Collobert, BERT (contextuel). (Faire visualisations avec T-SNE cf TP2).\n",
    "Evaluer embeddings avec datasets pré-établis et annotés manuellement comme dans le TP pour justifier du choix d'embedding. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Load the data with a dictionnary (text is the movie review \n",
    "# and label is the positive or negative label)\n",
    "def load_imdb_data(path, split=\"train\"):\n",
    "    data = []\n",
    "    for label in ['pos', 'neg']:\n",
    "        folder = os.path.join(path, split, label)\n",
    "        for filename in os.listdir(folder):\n",
    "            with open(os.path.join(folder, filename), encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "                data.append({\n",
    "                    'text': text,\n",
    "                    'label': 1 if label == 'pos' else 0\n",
    "                })\n",
    "    return data\n",
    "\n",
    "train_data = load_imdb_data(\"data_movie\", split=\"train\")\n",
    "test_data = load_imdb_data(\"data_movie\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50000 unsupervised reviews.\n"
     ]
    }
   ],
   "source": [
    "# Load the unsupervised data \n",
    "\n",
    "def load_unsupervised_data(path):\n",
    "    data = []\n",
    "    for filename in os.listdir(path):\n",
    "        with open(os.path.join(path, filename), encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "            data.append(text)  # store only text since they are unlabeled\n",
    "    return data\n",
    "\n",
    "unsupervised_data = load_unsupervised_data(\"data_movie/train/unsup\")\n",
    "print(f\"Loaded {len(unsupervised_data)} unsupervised reviews.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'A solid, if unremarkable film. Matthau, as Einstein, was wonderful. My favorite part, and the only thing that would make me go out of my way to see this again, was the wonderful scene with the physicists playing badmitton, I loved the sweaters and the conversation while they waited for Robbins to retrieve the birdie.', 'label': 1}\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "print(train_data[2])\n",
    "print(len(train_data)+len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some movies receive substantially more reviews than others so limit to 30 reviews from any movie in the collection. Initial paper did not use any tokenizer since BERT did not exist at the time so tokenized simply => can try to use BERT tokenizer now ? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization :\n",
    " - The paper used a simple tokenizer because BERT did not exist at the time so my first approach will be to replicate this. They build a fixed dictionary of the 5.000 most frequent tokens, but ignore the 50 most frequent terms from the original full vocabulary. They do not stem or remove stop words such as punctuation '!',':-)' since they induce sentiment. \n",
    " - Second approach : Using BERT tokenizer (or another) which has good results, because it gives contextualized embeddings (so richer), is pretrained on massive data, and has a better accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first approach, I use regex in order to make punctuation be a token because I could get things like : \n",
    "- \"wonderful!!!\" and if I don't split the punctuation, the token would be [\"wonderful!!!\"] while I would like it to be [\"wonderful\",\"!!!\"]\n",
    "- then I just split by spaces to get the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization : first approach same as in the paper, using a simple tokenizer\n",
    "# based on splitting words \n",
    "\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "class CustomTokenizer:\n",
    "    # the size of the vocabulary is 5000, and we skip the 50 most frequent tokens \n",
    "    def __init__(self, max_vocab_size=5000, skip_top=50):\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.skip_top = skip_top\n",
    "        self.vocab = {}\n",
    "        self.token_freqs = Counter() # for frequency\n",
    "        self.special_tokens = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'([!?.,:;()\"])', r' \\1 ', text)  # isolate punctuation \n",
    "        text = re.sub(r\"\\s+\", \" \", text)  # clean multiple spaces\n",
    "        return text.strip().split()\n",
    "\n",
    "    def build_vocab(self, texts):\n",
    "        # count all tokens\n",
    "        for text in texts:\n",
    "            tokens = self.tokenize(text)\n",
    "            self.token_freqs.update(tokens)\n",
    "\n",
    "        # build vocab\n",
    "        most_common = self.token_freqs.most_common(self.skip_top + self.max_vocab_size)\n",
    "        filtered_tokens = most_common[self.skip_top:]  # to skip the skip_top first tokens \n",
    "\n",
    "        # start vocab with special tokens for padding and unknown\n",
    "        self.vocab = dict(self.special_tokens)\n",
    "        for idx, (token, _) in enumerate(filtered_tokens, start=len(self.vocab)):\n",
    "            self.vocab[token] = idx \n",
    "\n",
    "    def encode(self, text):\n",
    "        tokens = self.tokenize(text)\n",
    "        return [self.vocab.get(t, self.vocab[\"<UNK>\"]) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Vocabulary Preview (first 50 words after special tokens) ---\n",
      "   2 : it's\n",
      "   3 : ?\n",
      "   4 : if\n",
      "   5 : some\n",
      "   6 : there\n",
      "   7 : what\n",
      "   8 : good\n",
      "   9 : more\n",
      "  10 : very\n",
      "  11 : when\n",
      "  12 : she\n",
      "  13 : even\n",
      "  14 : up\n",
      "  15 : no\n",
      "  16 : time\n",
      "  17 : would\n",
      "  18 : my\n",
      "  19 : which\n",
      "  20 : only\n",
      "  21 : really\n",
      "  22 : story\n",
      "  23 : their\n",
      "  24 : had\n",
      "  25 : see\n",
      "  26 : can\n",
      "  27 : were\n",
      "  28 : me\n",
      "  29 : :\n",
      "  30 : than\n",
      "  31 : we\n",
      "  32 : much\n",
      "  33 : -\n",
      "  34 : well\n",
      "  35 : been\n",
      "  36 : get\n",
      "  37 : will\n",
      "  38 : into\n",
      "  39 : bad\n",
      "  40 : people\n",
      "  41 : other\n",
      "  42 : because\n",
      "  43 : do\n",
      "  44 : also\n",
      "  45 : great\n",
      "  46 : him\n",
      "  47 : how\n",
      "  48 : first\n",
      "  49 : most\n",
      "  50 : don't\n",
      "  51 : made\n"
     ]
    }
   ],
   "source": [
    "# Tokenize from training data to prevent data leakage from the test data and \n",
    "# to allow better generalization\n",
    "\n",
    "train_texts = [sample['text'] for sample in train_data]\n",
    "tokenizer = CustomTokenizer()\n",
    "\n",
    "tokenizer.build_vocab(train_texts)\n",
    "\n",
    "print(\"\\n--- Vocabulary Preview (first 50 words after special tokens) ---\")\n",
    "for i, (word, idx) in enumerate(list(tokenizer.vocab.items())[2:52], start=1):\n",
    "    print(f\"{idx:4} : {word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unsupervised data : tokenization  \n",
    "\n",
    "unsup_tokenized = [tokenizer.tokenize(text) for text in unsupervised_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization seems to work well, and we still have the information of ['don't'] and not ['do','n't'] which is questionable for sentiment analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenization step is now done so we can encode the dataset to convert each text review into a sequence of integers (token ids) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the data \n",
    "\n",
    "encoded_train = []\n",
    "for sample in train_data:\n",
    "    encoded = tokenizer.encode(sample['text'])\n",
    "    encoded_train.append({\n",
    "        'input_ids': encoded,\n",
    "        'label': sample['label']\n",
    "    })\n",
    "\n",
    "encoded_test = []\n",
    "for sample in test_data:\n",
    "    encoded = tokenizer.encode(sample['text'])\n",
    "    encoded_test.append({\n",
    "        'input_ids': encoded,\n",
    "        'label': sample['label']\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [1100, 160, 1, 1101, 1, 747, 1511, 1, 2517, 1, 1, 1, 1, 264, 1, 1, 1, 1, 1285, 1, 1, 1, 1, 142, 1, 592, 1, 2241, 1, 1, 1, 1753, 1, 1, 1, 1, 973, 1, 1, 3273, 1, 1, 1, 1, 1, 1, 566, 1, 1, 98, 1, 1, 1, 1, 10, 34, 1931, 1, 1, 1249, 188, 1, 361, 1, 1, 1, 1, 1163, 1, 1, 4950, 1, 1, 11, 1, 1050, 38, 1, 902, 4186, 4951, 1, 1, 1, 1, 1, 1973, 2242, 1, 1, 380, 1800, 1, 1, 141, 21, 333, 1, 1, 1, 4684, 1, 539, 1, 1, 3705, 1, 1, 1, 1, 331, 1, 2009, 150, 1, 1, 1, 1, 1, 1, 131, 1, 1, 1, 1, 1, 45, 208, 1, 1, 1, 521, 1229, 1, 1, 1, 1, 705, 1, 1, 1, 1, 1, 871, 1, 1, 1, 1, 1, 1, 1, 7, 1, 2114, 1, 423, 1, 1, 1, 1, 4078, 1, 1, 1, 1, 1, 1, 1, 1, 3602, 1, 345, 1, 1, 777, 90, 34, 1, 136, 454, 2243, 1, 1, 1, 1, 1, 1, 235, 1, 2626, 1, 1, 1, 2115], 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "print(encoded_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a vocab file from the dataset but I do not use it for now since its size is way larger than 5000 (it must come from more reviews) and I will see first if my vocabulary is enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is encoded, the next step is to apply some padding in order to have the same length as input for the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length in the training data: 2719\n"
     ]
    }
   ],
   "source": [
    "# Padding based on the largest sentence (token wise)\n",
    "\n",
    "max_length = max(len(sample['input_ids']) for sample in encoded_train)\n",
    "print(f\"Maximum sequence length in the training data: {max_length}\")\n",
    "\n",
    "\n",
    "def pad_sequences(sequences, max_length, pad_token_id=0):\n",
    "    padded_sequences = []\n",
    "    for seq in sequences:\n",
    "        padding = [pad_token_id] * (max_length - len(seq))  # Pad to the right\n",
    "        padded_sequences.append(seq + padding if len(seq) < max_length else seq[:max_length])\n",
    "    return padded_sequences\n",
    "\n",
    "\n",
    "# Apply padding to the encoded data\n",
    "padded_encoded_train = pad_sequences([sample['input_ids'] for sample in encoded_train], max_length)\n",
    "padded_encoded_test = pad_sequences([sample['input_ids'] for sample in encoded_test], max_length)\n",
    "\n",
    "padded_train_data = [{'input_ids': seq, 'label': sample['label']} for seq, sample in zip(padded_encoded_train, encoded_train)]\n",
    "padded_test_data = [{'input_ids': seq, 'label': sample['label']} for seq, sample in zip(padded_encoded_test, encoded_test)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example padded train data: {'input_ids': [1, 1, 1, 1, 171, 15, 1129, 6, 209, 1, 1, 131, 1, 858, 4297, 3472, 1, 1, 1440, 1, 793, 1, 1, 78, 870, 1, 1, 121, 122, 1, 1, 1, 1, 1, 95, 1, 1, 1, 1, 69, 1, 1, 1486, 1978, 1, 65, 1, 1558, 1, 1, 1, 1, 1, 1712, 1, 1, 1, 521, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 1}\n",
      "Example padded test data: {'input_ids': [423, 1, 1, 720, 22, 1, 272, 1, 239, 1, 1592, 1, 1, 281, 1000, 1, 565, 632, 1, 463, 27, 1945, 1, 12, 1, 1, 1, 1, 1, 1862, 1, 1, 3165, 1, 1, 1, 1, 774, 415, 1, 1, 8, 279, 1, 36, 204, 1, 1, 1, 1, 1, 11, 1, 1, 1, 2517, 1, 1, 1, 12, 57, 1, 517, 1, 669, 1, 1, 774, 1, 1, 1, 860, 1, 744, 105, 319, 12, 57, 36, 1, 1, 1599, 1, 1, 1, 1, 281, 1, 1, 1, 2182, 1, 1, 220, 117, 12, 57, 2263, 1, 1, 12, 513, 1, 1153, 38, 1, 4132, 1, 1, 3260, 3551, 1, 1, 1, 743, 1488, 102, 1, 106, 1666, 1, 1, 1, 119, 1, 1, 28, 1, 1, 1, 1666, 494, 1, 1, 1, 1, 1, 1, 566, 3050, 1871, 1, 12, 1, 1005, 1, 1, 970, 1, 1, 1, 11, 1, 553, 1, 12, 24, 3897, 1, 1, 1, 1, 1341, 1, 12, 175, 1, 3260, 3551, 36, 3726, 1, 283, 1, 1, 1, 3337, 303, 12, 811, 1, 1793, 1, 1, 714, 1341, 1, 308, 1, 2192, 46, 1, 1, 15, 162, 1, 713, 1, 1, 1, 81, 1, 1, 2307, 1, 1, 1, 1, 1, 1, 211, 1, 1, 1, 1, 1, 1, 1, 1, 1, 81, 1, 555, 124, 1, 1, 1, 1, 266, 1, 1, 1, 1, 1, 127, 1, 67, 12, 1, 1, 266, 488, 1, 1, 1, 1, 3172, 22, 1, 1, 1, 353, 1, 31, 1, 4368, 1, 2200, 1, 1, 1, 2260, 1, 81, 170, 881, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Example padded train data: {padded_train_data[0]}\")\n",
    "print(f\"Example padded test data: {padded_test_data[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that dataset imdb is loaded, we can try to tokenize the text and build a vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now train our own embedding in a first approach, instead of using pre trained embeddings such as GLoVE, Collobert, BERT. I use Word2Vec to do so, on the unsupervised data. Then, I will use BERT which will likely result in better performance. \n",
    "We want 50-dimensional vectors (embeddings) in conformity with the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of the unsupervised embedding using Word2Vec\n",
    "\n",
    "# I set vector_size = 50 same as the dimensional vectors of the article\n",
    "model = Word2Vec(sentences=unsup_tokenized,vector_size=50,window=5,min_count=1,workers=4) \n",
    "model.save(\"word2vec_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for 'good': [-0.02668276  1.6304002   3.0908344   2.0695004  -2.6603909   5.886345\n",
      " -0.95599335 -3.1640143   0.51996505 -0.92471564 -1.0598339   0.092731\n",
      "  0.2507879   0.1548105  -2.5982487   1.6632124   1.1573884   0.46780157\n",
      "  0.41147286 -1.9868916  -0.9447748  -1.7200239   1.1644139  -0.01681888\n",
      " -3.199935   -0.80628073  2.8439891   3.559361   -2.7016542   1.7777083\n",
      "  1.4225734   3.1251025  -2.216038   -3.990622    2.8474953  -0.966506\n",
      " -3.1862013   5.037959   -0.9162124  -2.260319    1.9980825  -1.8151331\n",
      " -3.8939815  -1.5061668   2.380033   -6.5707107  -2.4918954  -4.733701\n",
      "  0.6118964  -3.5265934 ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test of the embedding \n",
    "word_vector = model.wv['good']\n",
    "print(f\"Embedding for 'good': {word_vector}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us test the modelled embedding thanks to cosine similarity and some query words : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine similarity function \n",
    "\n",
    "def cosine_similarity(word1, word2, model):\n",
    "    # Get word vectors for word1 and word2\n",
    "    vector1 = model.wv[word1]\n",
    "    vector2 = model.wv[word2]\n",
    "    \n",
    "    # Compute cosine similarity using scipy\n",
    "    return 1 - cosine(vector1, vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most similar words to 'melancholy':\n",
      "dreamy: 0.9010\n",
      "melancholic: 0.8834\n",
      "whimsical: 0.8388\n",
      "playful: 0.8354\n",
      "heartfelt: 0.8329\n",
      "\n",
      "Most similar words to 'ghastly':\n",
      "abundant: 0.7680\n",
      "noisy: 0.7659\n",
      "hairless: 0.7619\n",
      "cartoony: 0.7575\n",
      "rancid: 0.7555\n",
      "\n",
      "Most similar words to 'lackluster':\n",
      "sub-par: 0.8523\n",
      "unimaginative: 0.8485\n",
      "lacklustre: 0.8460\n",
      "leaden: 0.8353\n",
      "threadbare: 0.8342\n",
      "\n",
      "Most similar words to 'romantic':\n",
      "quirky: 0.7992\n",
      "romance: 0.7885\n",
      "erotic: 0.7753\n",
      "tender: 0.7512\n",
      "light-hearted: 0.7381\n"
     ]
    }
   ],
   "source": [
    "# Test with a query word such as 'romantic' like in the article \n",
    "\n",
    "query_words = [\"melancholy\",\"ghastly\",\"lackluster\",\"romantic\"]\n",
    "top_n = 5\n",
    "\n",
    "# Most similar words\n",
    "for w in query_words: \n",
    "    similar_words = model.wv.most_similar(w, topn=top_n)\n",
    "\n",
    "    print(f\"\\nMost similar words to '{w}':\")\n",
    "    for word, similarity in similar_words:\n",
    "        print(f\"{word}: {similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Word2Vec trained embedding seems to be quite good at finding similar words to the query words in entry !\n",
    "It captures word semantics based on co-occurrence statistics. However, it does not explicitly capture sentiment information. We will know refine embeddings to predict sentiment labels (positive or negative polarity) thanks to the labels (supervised part)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 27\u001b[0m\n\u001b[1;32m     23\u001b[0m         embeddings\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39marray(sentence_embedding))\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(embeddings)\n\u001b[0;32m---> 27\u001b[0m train_embedded \u001b[38;5;241m=\u001b[39m prepare_embeddings(padded_train_data, model)\n\u001b[1;32m     28\u001b[0m test_embedded \u001b[38;5;241m=\u001b[39m prepare_embeddings(padded_test_data, model)\n",
      "Cell \u001b[0;32mIn[19], line 19\u001b[0m, in \u001b[0;36mprepare_embeddings\u001b[0;34m(data, word2vec_model, max_sequence_length, embedding_dim)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token_id \u001b[38;5;129;01min\u001b[39;00m sentence[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Get word embedding for token\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m token_id \u001b[38;5;129;01min\u001b[39;00m word2vec_model\u001b[38;5;241m.\u001b[39mwv:\n\u001b[0;32m---> 19\u001b[0m         sentence_embedding\u001b[38;5;241m.\u001b[39mappend(word2vec_model\u001b[38;5;241m.\u001b[39mwv[token_id])\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     21\u001b[0m         sentence_embedding\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mzeros(embedding_dim))  \u001b[38;5;66;03m# OOV token (zero vector)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gensim/models/keyedvectors.py:403\u001b[0m, in \u001b[0;36mKeyedVectors.__getitem__\u001b[0;34m(self, key_or_keys)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get vector representation of `key_or_keys`.\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \n\u001b[1;32m    391\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    400\u001b[0m \n\u001b[1;32m    401\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key_or_keys, _KEY_TYPES):\n\u001b[0;32m--> 403\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_vector(key_or_keys)\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vstack([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_vector(key) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m key_or_keys])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gensim/models/keyedvectors.py:453\u001b[0m, in \u001b[0;36mKeyedVectors.get_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    451\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectors[index]\n\u001b[0;32m--> 453\u001b[0m result\u001b[38;5;241m.\u001b[39msetflags(write\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)  \u001b[38;5;66;03m# disallow direct tampering that would invalidate `norms` etc\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# I will use a logistic regression in order to have as an output the probability \n",
    "# of a word being associated with a positive sentiment. \n",
    "\n",
    "# First, let us embed the encoded train and test data \n",
    "\n",
    "# 1st try : crash of the kernel .... :/\n",
    "\n",
    "def prepare_embeddings(data, word2vec_model, max_sequence_length=max_length,embedding_dim=50):\n",
    "    \"\"\"\n",
    "    Convert the tokenized sentences into word embeddings.\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    \n",
    "    for sentence in data:\n",
    "        sentence_embedding = []\n",
    "        for token_id in sentence['input_ids']:\n",
    "            # Get word embedding for token\n",
    "            if token_id in word2vec_model.wv:\n",
    "                sentence_embedding.append(word2vec_model.wv[token_id])\n",
    "            else:\n",
    "                sentence_embedding.append(np.zeros(embedding_dim))  # OOV token (zero vector)\n",
    "        \n",
    "        embeddings.append(np.array(sentence_embedding))\n",
    "    \n",
    "    return np.array(embeddings)\n",
    "\n",
    "train_embedded = prepare_embeddings(padded_train_data, model)\n",
    "test_embedded = prepare_embeddings(padded_test_data, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd try : proceed in batches : WORKS !!! \n",
    "\n",
    "def prepare_embeddings(data, word2vec_model, max_sequence_length=100, embedding_dim=50, batch_size=1000):\n",
    "    \"\"\"\n",
    "    Convert the tokenized sentences into word embeddings.\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    \n",
    "    # Process in batches to avoid memory overload\n",
    "    for start_idx in range(0, len(data), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(data))\n",
    "        batch = data[start_idx:end_idx]\n",
    "        \n",
    "        batch_embeddings = []\n",
    "        \n",
    "        for sentence in batch:\n",
    "            sentence_embedding = []\n",
    "            \n",
    "            for token_id in sentence['input_ids']:\n",
    "                # Check if the token exists in the Word2Vec model\n",
    "                if token_id in word2vec_model.wv.key_to_index:\n",
    "                    sentence_embedding.append(word2vec_model.wv[token_id])\n",
    "                else:\n",
    "                    sentence_embedding.append(np.zeros(embedding_dim))  # OOV token (zero vector)\n",
    "            \n",
    "            # Ensure sequence length matches max_sequence_length\n",
    "            padding_length = max_sequence_length - len(sentence_embedding)\n",
    "            if padding_length > 0:\n",
    "                sentence_embedding.extend([np.zeros(embedding_dim)] * padding_length)\n",
    "            else:\n",
    "                sentence_embedding = sentence_embedding[:max_sequence_length]\n",
    "            \n",
    "            batch_embeddings.append(np.array(sentence_embedding))\n",
    "        \n",
    "        embeddings.extend(batch_embeddings)\n",
    "    \n",
    "    return np.array(embeddings)\n",
    "\n",
    "\n",
    "train_embedded = prepare_embeddings(padded_train_data, model)\n",
    "test_embedded = prepare_embeddings(padded_test_data, model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSA to try ! Based only on semantic analysis !!! and at the end compare the three approaches. Maybe change parameters in Word2Vec and see if get better ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def encode_text_with_embeddings(text, model, tokenizer):\n",
    "    # Tokenize the text\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    # Get the corresponding embeddings for each token\n",
    "    embeddings = [model.wv[token] if token in model.wv else np.zeros(model.vector_size) for token in tokens]\n",
    "    return embeddings\n",
    "\n",
    "# Encode the training and testing data\n",
    "encoded_train_data = [encode_text_with_embeddings(text, model, tokenizer) for text in train_data]\n",
    "encoded_test_data = [encode_text_with_embeddings(text, model, tokenizer) for text in test_data]\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def pad_sequences_to_max_length(sequences, max_len):\n",
    "    # Pad the sequences to a fixed length (max_len)\n",
    "    return pad_sequences(sequences, maxlen=max_len, dtype=\"float32\", padding=\"post\", value=0)\n",
    "\n",
    "# Choose a max length based on your data (e.g., the longest sentence)\n",
    "max_len = max(len(seq) for seq in encoded_train_data)\n",
    "\n",
    "# Pad the sequences\n",
    "padded_train_data = pad_sequences_to_max_length(encoded_train_data, max_len)\n",
    "padded_test_data = pad_sequences_to_max_length(encoded_test_data, max_len)\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Create an LSTM model for sentiment analysis\n",
    "model = Sequential([\n",
    "    LSTM(128, input_shape=(max_len, model.vector_size), return_sequences=False),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model on your padded data\n",
    "model.fit(padded_train_data, np.array(train_labels), epochs=5, batch_size=32, validation_data=(padded_test_data, np.array(test_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization bis: \n",
    "\n",
    "class IMDbDataset(Dataset):\n",
    "    def __init__(self, directory, vocab=None, max_vocab_size=20000, min_freq=2):\n",
    "        self.samples = []  # List of (tokens, label)\n",
    "        self.vocab = vocab\n",
    "        self.pad_token = \"<PAD>\" # padding token : maked all sequences in a batch the same length so they can be stacked into tensors = add padding (so extra zeros to shorter sequences)\n",
    "        self.unk_token = \"<UNK>\" # unknown token : handes any word that is not in the vocab (bc our vocab is max 20.000 words)\n",
    "\n",
    "        for label, label_val in [(\"pos\", 1), (\"neg\", 0)]:\n",
    "            folder = os.path.join(directory, label)\n",
    "            for filename in os.listdir(folder):\n",
    "                with open(os.path.join(folder, filename), encoding=\"utf-8\") as f:\n",
    "                    text = f.read()\n",
    "                    tokens = self.tokenize(text)\n",
    "                    self.samples.append((tokens, label_val))\n",
    "\n",
    "        if self.vocab is None:\n",
    "            all_tokens = [token for tokens, _ in self.samples for token in tokens]\n",
    "            freqs = Counter(all_tokens)\n",
    "            most_common = [\n",
    "                (word, freq) for word, freq in freqs.items() if freq >= min_freq\n",
    "            ]\n",
    "            most_common = sorted(most_common, key=lambda x: -x[1])[:max_vocab_size]\n",
    "            self.vocab = {\n",
    "                word: idx + 2 for idx, (word, _) in enumerate(most_common)\n",
    "            }  # Reserve 0, 1\n",
    "            self.vocab[self.pad_token] = 0\n",
    "            self.vocab[self.unk_token] = 1\n",
    "\n",
    "        self.pad_idx = self.vocab[self.pad_token]\n",
    "        self.unk_idx = self.vocab[self.unk_token]\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"<.*?>\", \"\", text)\n",
    "        return re.findall(r\"\\b\\w+\\b\", text)\n",
    "\n",
    "    def encode(self, tokens):\n",
    "        return [self.vocab.get(token, self.unk_idx) for token in tokens]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens, label = self.samples[idx]\n",
    "        encoded = self.encode(tokens)\n",
    "        return torch.tensor(encoded), torch.tensor(label)\n",
    "\n",
    "def collate_batch(batch):\n",
    "    sequences, labels = zip(*batch)\n",
    "    lengths = [len(seq) for seq in sequences]\n",
    "    max_len = max(lengths)\n",
    "    padded = [torch.cat([seq, torch.zeros(max_len - len(seq))]) for seq in sequences]\n",
    "    return torch.stack(padded).long(), torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step : model - average of word embeddings and logistic regression \n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class AvgEmbeddingsClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.classifier = nn.Linear(embedding_dim, 1)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embedded = self.embedding(input_ids)  # (batch, seq_len, emb_dim)\n",
    "        mask = (input_ids != 0).unsqueeze(-1)  # padding mask\n",
    "        summed = torch.sum(embedded * mask, dim=1)\n",
    "        counts = torch.sum(mask, dim=1)\n",
    "        avg = summed / counts\n",
    "        logits = self.classifier(avg).squeeze(1)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy + evaluation \n",
    "\n",
    "def binary_accuracy(preds, labels):\n",
    "    preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (preds == labels).float()\n",
    "    return correct.sum() / len(correct)\n",
    "\n",
    "def evaluate_model(model, loader, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    total_acc = 0\n",
    "    total_loss = 0\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            inputs, labels = [x.to(device) for x in batch]\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels.float())\n",
    "            acc = binary_accuracy(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_acc += acc.item()\n",
    "\n",
    "    print(f\" → Val Loss: {total_loss / len(loader):.4f} | Acc: {total_acc / len(loader):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function \n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs=5, lr=1e-3, device=\"cpu\"):\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            inputs, labels = [x.to(device) for x in batch]\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels.float())\n",
    "            acc = binary_accuracy(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        avg_acc = epoch_acc / len(train_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {avg_loss:.4f} | Acc: {avg_acc:.4f}\")\n",
    "\n",
    "        evaluate_model(model, val_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap up \n",
    "\n",
    "# Paths\n",
    "train_path = \"data_movie/train\"\n",
    "test_path = \"data_movie/test\"\n",
    "\n",
    "# Load data\n",
    "train_dataset = IMDbDataset(train_path)\n",
    "test_dataset = IMDbDataset(test_path, vocab=train_dataset.vocab)  # use same vocab\n",
    "\n",
    "# Split train/val\n",
    "random.shuffle(train_dataset.samples)\n",
    "split_idx = int(0.9 * len(train_dataset))\n",
    "train_data = torch.utils.data.Subset(train_dataset, range(split_idx))\n",
    "val_data = torch.utils.data.Subset(train_dataset, range(split_idx, len(train_dataset)))\n",
    "\n",
    "# Dataloaders\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True, collate_fn=collate_batch)\n",
    "val_loader = DataLoader(val_data, batch_size=32, collate_fn=collate_batch)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, collate_fn=collate_batch)\n",
    "\n",
    "# Model\n",
    "vocab_size = len(train_dataset.vocab)\n",
    "model = AvgEmbeddingsClassifier(vocab_size, embedding_dim=100)\n",
    "\n",
    "# Train\n",
    "train_model(model, train_loader, val_loader, epochs=5, lr=1e-3, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Evaluate \n",
    "\n",
    "evaluate_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Preprocessing\n",
    "   ⤷ Tokenize reviews\n",
    "   ⤷ Build vocab\n",
    "\n",
    "2. Word Vector Initialization\n",
    "   ⤷ Random or GloVe init\n",
    "   ⤷ Embeddings will be fine-tuned via supervision\n",
    "\n",
    "3. Context Window or Full Review\n",
    "   ⤷ Either train on word windows (as in the paper)\n",
    "   ⤷ Or process whole review as a sequence\n",
    "\n",
    "4. Classifier\n",
    "   ⤷ Logistic regression (window-level)\n",
    "   ⤷ OR CNN/LSTM over full review\n",
    "\n",
    "5. Training\n",
    "   ⤷ Cross-entropy loss\n",
    "   ⤷ SGD or Adam\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
