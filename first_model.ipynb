{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import keras\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import re\n",
    "import random\n",
    "from gensim.models import Word2Vec\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from scipy.sparse import lil_matrix\n",
    "import matplotlib.pyplot as plt \n",
    "import plotly.graph_objects as go\n",
    "from tokenizer import CustomTokenizer\n",
    "from wordcloud import WordCloud\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import log_loss\n",
    "from utils import plot_wordclouds_by_label, plot_review_length_distribution_plotly, plot_text_length_histograms, describe_text_data,longest_and_shortest_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> Pre training word vectors (embeddings) using unlabeled data = unsupervised manner.\n",
    "Model : similar to Word2Vec to learn distributed representations of words (allow to capture semantic relationships between words and synonyms in similar contexts). \n",
    "-> Supervised learning using labeled data : word embeddings previously learnt as input features for training sentiment classifier, which was a logistic regression (permits the classifier to learn word embedding to map reviews to sentiment labels). Step for fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectif de l'article : \n",
    " Apprendre des word vectors non pas via des méthodes non supervisées classiques (comme Word2Vec), mais en les supervisant directement via une tâche de classification de sentiment.\n",
    "\n",
    " Mise en place modèle :\n",
    " - 1 : Tenter approche non supervisée classique Word2Vec classique et observer résultats (moyens?)\n",
    " - 2 : Approche de l'article \n",
    " - 3 : Trouver d'autres méthodes état de l'art et tester pour comparer ? \n",
    "\n",
    " Dataset : \n",
    " - phrases et sous-phrases extraites de critiques de film (Rotten Tomatoes)\n",
    " - chaque sous-phrase annotée avec score sentiment (fine-grained ou binaire)\n",
    " \n",
    "\n",
    " Approche principale : \n",
    " Utilisation modèle récursif (RNN) structuré selon grammaire phrases\n",
    " Chaque mot représenté par vecteur\n",
    " Vecteurs combinés récursivement selon structure syntaxique (parse tree) pour produire représentation de phrase.\n",
    " Supervision à chaque noeud de l'arbre = permet ajuster vecteurs de mot selon contribution au sentiment \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A répliquer : \n",
    "- Prétraitement : tokenisation, parsing syntaxique (parser de constituants type Stanford Parser), extraction de toutes sous phrases (phrases, clauses, etc.)\n",
    "- Modèle : Implémenter RNN sur arbres syntaxiques (matrice embedding, fonction de composition f(W[v1;v2] +b), classificateur au dessus représnetations des noeuds pour prédire sentiment)\n",
    "- Entraînement : loss supervisée à chaque noeud (cross-entropy), propagation gradient à travers recherche recursive "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "implémentation : https://github.com/stanfordnlp/treelstm\n",
    "\n",
    "Comparer la perf avec approches modernes : LSTM, BERT, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Stanford Sentiment Treebank is a corpus with fully labeled parse trees that allows for a complete analysis of the compositional effects of sentiment in language. The corpus is based on the dataset introduced by Pang and Lee (2005) and consists of 11,855 single sentences extracted from movie reviews. It was parsed with the Stanford parser and includes a total of 215,154 unique phrases from those parse trees, each annotated by 3 human judges.\n",
    "\n",
    "Each phrase is labelled as either negative, somewhat negative, neutral, somewhat positive or positive. The corpus with all 5 labels is referred to as SST-5 or SST fine-grained. Binary classification experiments on full sentences (negative or somewhat negative vs somewhat positive or positive with neutral sentences discarded) refer to the dataset as SST-2 or SST binary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lMD \n",
    "- 25,000 labeled reviews for training\n",
    "(Highly polar — i.e., very positive or very negative)\n",
    "- 25,000 labeled reviews for testing\n",
    "- 50,000 unlabeled reviews for unsupervised pre-training\n",
    "- Raw text and bag-of-words format included\n",
    "- Binary classification only (positive vs. negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb  # preprocessed version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "sst_dataset = load_dataset(\"sst\", \"default\")  # pour fine-grained\n",
    "# Ou : load_dataset(\"sst2\") pour version binaire\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data with a dictionnary (text is the movie review \n",
    "# and label is the positive or negative label)\n",
    "def load_imdb_data(path, split=\"train\"):\n",
    "    data = []\n",
    "    for label in ['pos', 'neg']:\n",
    "        folder = os.path.join(path, split, label)\n",
    "        for filename in os.listdir(folder):\n",
    "            with open(os.path.join(folder, filename), encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "                data.append({\n",
    "                    'text': text,\n",
    "                    'label': 1 if label == 'pos' else 0\n",
    "                })\n",
    "    return data\n",
    "\n",
    "train_data = load_imdb_data(\"data_movie\", split=\"train\")\n",
    "test_data = load_imdb_data(\"data_movie\", split=\"test\")\n",
    "print(f\"Loaded {len(train_data)} train reviews.\")\n",
    "print(f\"Loaded {len(test_data)} test reviews.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data[2])\n",
    "print(len(train_data)+len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Descriptive analysis** \n",
    "\n",
    "First, let us carry a descriptive analysis of the train and test set (and maybe one of the unsupervised set as well since it is used next). \n",
    "The goal is to see basic statistics (number of samples, disctribution of labels,average length text in tokens), but also textual insights (the most frequent tokens, vocabulary size, longest/shortest reviews and possible outliers), and finally some global visualizations (histogram of text length, bar plot of label distribution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = [sample['text'] for sample in train_data]\n",
    "train_labels = [sample['label'] for sample in train_data]\n",
    "test_texts = [sample['text'] for sample in test_data]\n",
    "test_labels = [sample['label'] for sample in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization : first approach same as in the paper, using a simple tokenizer\n",
    "# based on splitting words \n",
    "desc_df = describe_text_data(train_data, test_data, tokenizer=CustomTokenizer())\n",
    "desc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = CustomTokenizer()\n",
    "tokenizer.build_vocab(train_texts + test_texts)\n",
    "\n",
    "\n",
    "# --- Most frequent tokens ---\n",
    "\n",
    "top_tokens = tokenizer.token_freqs.most_common(50)\n",
    "print(\"\\nTop 50 Most Common Tokens:\")\n",
    "for token, freq in top_tokens:\n",
    "    print(f\"{token:>10} : {freq}\")\n",
    "\n",
    "# --- Vocabulary Overlap ---\n",
    "train_vocab = set(t for text in train_texts for t in tokenizer.tokenize(text))\n",
    "test_vocab = set(t for text in test_texts for t in tokenizer.tokenize(text))\n",
    "shared_vocab = train_vocab & test_vocab\n",
    "\n",
    "print(f\"\\nVocab Overlap: {len(shared_vocab)} shared tokens\")\n",
    "print(f\"Train-only tokens: {len(train_vocab - test_vocab)}\")\n",
    "print(f\"Test-only tokens:  {len(test_vocab - train_vocab)}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Longest/Shortest Reviews ---\n",
    "longest_and_shortest_reviews(train_data,tokenizer = CustomTokenizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of the length of the reviews \n",
    "plot_text_length_histograms(train_data,test_data,tokenizer=CustomTokenizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First test of wordcloud\n",
    "plot_wordclouds_by_label(train_texts, train_labels,tokenizer=CustomTokenizer())\n",
    "\n",
    "# And if we want to add more stop words to see a bit more of the polarity \n",
    "plot_wordclouds_by_label(train_texts, train_labels,tokenizer=CustomTokenizer(),stop_words={\"br\", \"the\", \"and\", \"is\", \"it\", \"to\", \"of\",\"movie\",\"film\",\"one\",\"character\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wordcloud is strange since there is no huge difference between the negative reviews (label 0) and the positive ones (label 1)... That might explain the poor results of the model later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the link between the length of the review and the label \n",
    "plot_review_length_distribution_plotly(train_data, tokenizer=CustomTokenizer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Unsupervised part**\n",
    "\n",
    "I use the unsupervised part of the train set in order to learn word embeddings. I will compare the results of known embeddings such as Word2Vec, GloVe, and FastText, Collobert, BERT (contextuel). (Faire visualisations avec T-SNE cf TP2).\n",
    "Evaluer embeddings avec datasets pré-établis et annotés manuellement comme dans le TP pour justifier du choix d'embedding. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the unsupervised data \n",
    "\n",
    "def load_unsupervised_data(path):\n",
    "    data = []\n",
    "    for filename in os.listdir(path):\n",
    "        with open(os.path.join(path, filename), encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "            data.append(text)  # store only text since they are unlabeled\n",
    "    return data\n",
    "\n",
    "unsupervised_data = load_unsupervised_data(\"data_movie/train/unsup\")\n",
    "print(f\"Loaded {len(unsupervised_data)} unsupervised reviews.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization :\n",
    " - The paper used a simple tokenizer because BERT did not exist at the time so my first approach will be to replicate this. They build a fixed dictionary of the 5.000 most frequent tokens, but ignore the 50 most frequent terms from the original full vocabulary. They do not stem or remove stop words such as punctuation '!',':-)' since they induce sentiment. \n",
    " - Second approach : Using BERT tokenizer (or another) which has good results, because it gives contextualized embeddings (so richer), is pretrained on massive data, and has a better accuracy. \n",
    "\n",
    " !!! Limit to 30 reviews per movie !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First approach : building a custom tokenizer**\n",
    "For the first approach, I use regex in order to make punctuation be a token because I could get things like : \n",
    "- \"wonderful!!!\" and if I don't split the punctuation, the token would be [\"wonderful!!!\"] while I would like it to be [\"wonderful\",\"!!!\"]\n",
    "- then I just split by spaces to get the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize from training data to prevent data leakage from the test data and \n",
    "# to allow better generalization\n",
    "\n",
    "train_texts = [sample['text'] for sample in train_data]\n",
    "tokenizer = CustomTokenizer()\n",
    "\n",
    "vocab_dict = tokenizer.build_vocab(train_texts)\n",
    "\n",
    "\n",
    "print(\"\\n--- Vocabulary Preview (first 50 words after special tokens) ---\")\n",
    "for i, (word, idx) in enumerate(list(tokenizer.vocab.items())[2:52], start=1):\n",
    "    print(f\"{idx:4} : {word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unsupervised data : tokenization  \n",
    "\n",
    "unsup_tokenized = [tokenizer.tokenize(text) for text in unsupervised_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsup_tokenized[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization seems to work well, and we still have the information of ['don't'] and not ['do','n't'] which is questionable for sentiment analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenization step is now done so we can encode the dataset to convert each text review into a sequence of integers (token ids) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the data \n",
    "\n",
    "encoded_train = []\n",
    "for sample in train_data:\n",
    "    encoded = tokenizer.encode(sample['text'])\n",
    "    encoded_train.append({\n",
    "        'input_ids': encoded,\n",
    "        'label': sample['label']\n",
    "    })\n",
    "\n",
    "encoded_test = []\n",
    "for sample in test_data:\n",
    "    encoded = tokenizer.encode(sample['text'])\n",
    "    encoded_test.append({\n",
    "        'input_ids': encoded,\n",
    "        'label': sample['label']\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoded_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a vocab file from the dataset but I do not use it for now since its size is way larger than 5000 (it must come from more reviews) and I will see first if my vocabulary is enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is encoded, the next step is to apply some padding in order to have the same length as input for the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding based on the largest sentence (token wise)\n",
    "\n",
    "max_length = max(len(sample['input_ids']) for sample in encoded_train)\n",
    "print(f\"Maximum sequence length in the training data: {max_length}\")\n",
    "\n",
    "\n",
    "def pad_sequences(sequences, max_length, pad_token_id=0):\n",
    "    padded_sequences = []\n",
    "    for seq in sequences:\n",
    "        padding = [pad_token_id] * (max_length - len(seq))  # Pad to the right\n",
    "        padded_sequences.append(seq + padding if len(seq) < max_length else seq[:max_length])\n",
    "    return padded_sequences\n",
    "\n",
    "\n",
    "# Apply padding to the encoded data\n",
    "padded_encoded_train = pad_sequences([sample['input_ids'] for sample in encoded_train], max_length)\n",
    "padded_encoded_test = pad_sequences([sample['input_ids'] for sample in encoded_test], max_length)\n",
    "\n",
    "padded_train_data = [{'input_ids': seq, 'label': sample['label']} for seq, sample in zip(padded_encoded_train, encoded_train)]\n",
    "padded_test_data = [{'input_ids': seq, 'label': sample['label']} for seq, sample in zip(padded_encoded_test, encoded_test)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Example padded train data: {padded_train_data[0]}\")\n",
    "print(f\"Example padded test data: {padded_test_data[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that dataset imdb is loaded, we can try to tokenize the text and build a vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now train our own embedding in a first approach, instead of using pre trained embeddings such as GLoVE, Collobert, BERT. I use Word2Vec to do so, on the unsupervised data. Then, I will use BERT which will likely result in better performance. \n",
    "We want 50-dimensional vectors (embeddings) in conformity with the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of the unsupervised embedding using Word2Vec\n",
    "\n",
    "# I set vector_size = 50 same as the dimensional vectors of the article \n",
    "#and min_count = 5 because they filter words that appear less than 5 times \n",
    "\n",
    "model = Word2Vec(sentences=unsup_tokenized,vector_size=50,window=5,min_count=5,workers=4) \n",
    "model.save(\"word2vec_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test of the embedding \n",
    "word_vector = model.wv['good']\n",
    "print(f\"Embedding for 'good': {word_vector}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us test the modelled embedding thanks to cosine similarity and some query words : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine similarity function \n",
    "\n",
    "def cosine_similarity(word1, word2, model):\n",
    "    # Get word vectors for word1 and word2\n",
    "    vector1 = model.wv[word1]\n",
    "    vector2 = model.wv[word2]\n",
    "    \n",
    "    # Compute cosine similarity using scipy\n",
    "    return 1 - cosine(vector1, vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a query word such as 'romantic' like in the article \n",
    "\n",
    "query_words = [\"sadness\",\"witty\",\"dull\",\"romantic\"]\n",
    "top_n = 5\n",
    "\n",
    "# Most similar words\n",
    "for w in query_words: \n",
    "    similar_words = model.wv.most_similar(w, topn=top_n)\n",
    "\n",
    "    print(f\"\\nMost similar words to '{w}':\")\n",
    "    for word, similarity in similar_words:\n",
    "        print(f\"{word}: {similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Word2Vec trained embedding seems to be quite good at finding similar words to the query words in entry !\n",
    "It captures word semantics based on co-occurrence statistics. However, it does not explicitly capture sentiment information. We will know refine embeddings to predict sentiment labels (positive or negative polarity) thanks to the labels (supervised part)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Other word representations**\n",
    "\n",
    "- LSA : Another approach consists in Latent Semantic Analysis (LSA). Here, is applied a SVD to a tf.idf weighted, cosine normalized count matrix. \n",
    "- LDA : Latent Dirichlet Allocation. \n",
    "\n",
    "LSA uses tf-idf ie a heuristic weighting scheme based on frequency and inverse document frequency so not probabilistic contrary to LDA that uses raw term counts to assume documents are mixtures of topics, and each topic a distribution of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'idée est d'abord de créer une matrice de termes (tf-idf), puis de la réduire via SVD (Singular Value Decomposition) pour capturer les structures sémantiques latentes, tout en réduisant la dimensionnalité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer la matrice TF-IDF\n",
    "tokenizer = CustomTokenizer()\n",
    "vectorizer = TfidfVectorizer(tokenizer=tokenizer.tokenize)\n",
    "\n",
    "X = vectorizer.fit_transform(train_texts)  # Matrice TF-IDF\n",
    "\n",
    "#norm_X = X / np.linalg.norm(X, axis=1, keepdims=True)\n",
    "# crash ici car matrice X a de grandes chances d'être sparse\n",
    "\n",
    "# donc à la place on cherche à normaliser avc sklearn \n",
    "# Normalisation \n",
    "\n",
    "norm_X = normalize(X, norm='l2', axis=1)\n",
    "\n",
    "# SVD décomposition \n",
    "\n",
    "n_components = 100  # (100-300 courant pour LSA)\n",
    "svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
    "X_svd = svd.fit_transform(norm_X)\n",
    "\n",
    "# Extraction vecteurs de mots \n",
    "# Vocabulaire (mot -> index) tel que vu par le TfidfVectorizer\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "word_to_index_lsa = {word: idx for idx, word in enumerate(vocab)}\n",
    "\n",
    "# On transpose pour avoir une représentation \"mot-vecteur\" (LSA)\n",
    "# X_svd est document × composantes → on veut terme × composantes\n",
    "term_vectors_lsa = svd.components_.T  # shape: (n_terms, n_components)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sur query words \n",
    "#from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "query_words = [\"sadness\", \"witty\", \"dull\", \"romantic\"]\n",
    "top_n = 5\n",
    "\n",
    "for query_word in query_words:\n",
    "    if query_word not in word_to_index_lsa:\n",
    "        print(f\"'{query_word}' n'est pas dans le vocabulaire.\")\n",
    "        continue\n",
    "\n",
    "    query_idx = word_to_index_lsa[query_word]\n",
    "    query_vector = term_vectors_lsa[query_idx].reshape(1, -1)\n",
    "\n",
    "    # Calculer similarité cosinus entre ce mot et tous les autres\n",
    "    similarities = cosine_similarity(query_vector, term_vectors_lsa)[0]\n",
    "\n",
    "    # Obtenir les indices des mots les plus similaires\n",
    "    similar_indices = similarities.argsort()[::-1][1:top_n + 1]  # on skip le mot lui-même\n",
    "\n",
    "    print(f\"\\nMost similar words to '{query_word}':\")\n",
    "    for idx in similar_indices:\n",
    "        print(f\"{vocab[idx]}: {similarities[idx]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **LDA** \n",
    "The ultimate test I wanted to implement is to try the LDA approach (Latent Dirichlet Allocation). It works almost like LSA but while LSA produces a continuous vector space, LDA produces probability distributions on topics (discreet and interpretable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lila/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning:\n",
      "\n",
      "The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = CustomTokenizer()\n",
    "tokenizer.build_vocab(train_texts)\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=tokenizer.tokenize, max_features=5000)\n",
    "X_counts = vectorizer.fit_transform(train_texts)\n",
    "\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "word_to_index_lda = {word: idx for idx, word in enumerate(vocab)}\n",
    "\n",
    "n_topics = 50  # nombre de topics selon l'article\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "lda.fit(X_counts)\n",
    "\n",
    "# Shape: (n_topics, n_words)\n",
    "topic_word_distrib_lda = lda.components_  # nombre de fois qu'un mot est généré par un topic\n",
    "\n",
    "# On normalise pour obtenir une distribution de probas (somme = 1)\n",
    "word_topic_vectors_lda = topic_word_distrib_lda.T / topic_word_distrib_lda.T.sum(axis=1, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "query_words = [\"sadness\", \"witty\", \"dull\", \"romantic\"]\n",
    "top_n = 5\n",
    "\n",
    "for query_word in query_words:\n",
    "    if query_word not in word_to_index_lda:\n",
    "        print(f\"'{query_word}' n'est pas dans le vocabulaire.\")\n",
    "        continue\n",
    "\n",
    "    query_idx = word_to_index_lda[query_word]\n",
    "    query_vector = word_topic_vectors_lda[query_idx].reshape(1, -1)\n",
    "\n",
    "    similarities = cosine_similarity(query_vector, word_topic_vectors_lda)[0]\n",
    "    similar_indices = similarities.argsort()[::-1][1:top_n+1]\n",
    "\n",
    "    print(f\"\\nMost similar words to '{query_word}' (LDA-based):\")\n",
    "    for idx in similar_indices:\n",
    "        print(f\"{vocab[idx]}: {similarities[idx]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the words seem strange or even unexistant, which is because the words closest are taken from the built vocabulary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in order to incorporate the sentiment part, we should modify the learnt word vectors to be sensitive to sentiment. \n",
    "\n",
    "So the next step is to train a supervised sentiment classifier trained on the train and test datasets. We will then fine tune the initial embeddings from the unsupervised methods (Word2Vec, LSA, LDA). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Sentiment features from word embeddings** \n",
    "\n",
    "- First, let us average the word embeddings for each review (with Word2Vec, LSA and LDA, depending on what gives the best results) to obtain a sentence-level feature vector.\n",
    "- Then, we will try more advanced methods not mentionned in the paper (sentence transformers or tokenizers such as BERT,...) to obtain better sentence representations.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CustomTokenizer()\n",
    "train_tokenized = [tokenizer.tokenize(text) for text in train_texts]\n",
    "test_tokenized = [tokenizer.tokenize(text) for text in test_texts]\n",
    "\n",
    "y_train = train_labels\n",
    "y_test = test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the Word2Vec model \n",
    "word2vec_model = Word2Vec.load(\"word2vec_model\")\n",
    "\n",
    "def get_sentence_embedding(sentence, model):\n",
    "    embeddings = []\n",
    "    for word in sentence:\n",
    "        if word in model.wv:  # check that the word is in the vocabulary\n",
    "            embeddings.append(model.wv[word])  \n",
    "    \n",
    "    if len(embeddings) == 0:\n",
    "        return np.zeros(model.vector_size) # otherwise, returns a zero vector\n",
    "    \n",
    "    return np.mean(embeddings, axis=0) \n",
    "\n",
    "X_train_word2vec = np.array([get_sentence_embedding(sen, word2vec_model) for sen in train_tokenized])\n",
    "X_test_word2vec = np.array([get_sentence_embedding(sen, word2vec_model) for sen in test_tokenized])\n",
    "\n",
    "X_train_word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_zero_vector(vec):\n",
    "    return np.all(vec == 0)\n",
    "\n",
    "num_zero_vectors = sum(is_zero_vector(vec) for vec in X_train_word2vec)\n",
    "\n",
    "total_reviews = len(X_train_word2vec)\n",
    "\n",
    "# Percentage of zero vectors (to check that vocabulary is rich enough)\n",
    "percentage_zero = (num_zero_vectors / total_reviews) * 100\n",
    "print(f\"Number of zero vectors: {num_zero_vectors}/{total_reviews} ({percentage_zero:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Use logistic regression bc vectors are dense and continuous, \n",
    "# so work well on very dense, low-dim input \n",
    "\n",
    "# Train a logistic regression model\n",
    "classifier = LogisticRegression(max_iter=1000)\n",
    "classifier.fit(X_train_word2vec, y_train)\n",
    "\n",
    "# Evaluate the classifier on the training set (or on a validation set if you have one)\n",
    "y_pred = classifier.predict(X_test_word2vec)\n",
    "accuracy = accuracy_score(y_train, y_pred)\n",
    "\n",
    "print(f\"Test accuracy with Word2Vec embeddings: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a really good score with the logistic regression and word2vec model !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lila/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning:\n",
      "\n",
      "The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(25000, 100)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Créer la matrice TF-IDF\n",
    "tokenizer = CustomTokenizer()\n",
    "vectorizer = TfidfVectorizer(tokenizer=tokenizer.tokenize)\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(train_texts)  # Matrice TF-IDF\n",
    "\n",
    "norm_X = normalize(X_train_tfidf, norm='l2', axis=1)\n",
    "\n",
    "# SVD décomposition \n",
    "\n",
    "n_components = 100  # (100-300 courant pour LSA)\n",
    "lsa_model = TruncatedSVD(n_components=n_components, random_state=42)\n",
    "X_train_lsa = lsa_model.fit_transform(norm_X)\n",
    "\n",
    "# Here I use .transform(), and not .fit_transform() because the model is already fitted\n",
    "X_test_tfidf = vectorizer.transform(test_texts)  \n",
    "norm_X_test = normalize(X_test_tfidf, norm='l2', axis=1)\n",
    "X_test_lsa = lsa_model.transform(norm_X_test)\n",
    "\n",
    "#Shape (25.000,100) so logical with 25.000 reviews compressed into a 100-dimensional space \n",
    "X_train_lsa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy with LSA embeddings: 0.8428\n"
     ]
    }
   ],
   "source": [
    "# Train a logistic regression classifier using the LSA features\n",
    "classifier_lsa = LogisticRegression(max_iter=1000, C=1.0) # C is a regularization adjustment\n",
    "classifier_lsa.fit(X_train_lsa, y_train)\n",
    "\n",
    "\n",
    "y_pred_lsa = classifier_lsa.predict(X_test_lsa)\n",
    "accuracy_lsa = accuracy_score(y_test, y_pred_lsa)\n",
    "\n",
    "print(f\"Test accuracy with LSA embeddings: {accuracy_lsa:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8568, 0.8456, 0.8434, 0.8368, 0.8414])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(classifier_lsa, X_train_lsa, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test accuracy and cross-validation scores seem encouraging for LSA as well ! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Since I had already fit the LDA model, let us do the transform directly \n",
    "\n",
    "# Topic distributions on the training and test set \n",
    "X_train_lda = lda.transform(X_counts)\n",
    "X_test_counts = vectorizer.transform(test_texts) \n",
    "X_test_lda = lda.transform(X_test_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy with LDA embeddings: 0.8074\n"
     ]
    }
   ],
   "source": [
    "# Train a logistic regression classifier using the LDA features\n",
    "classifier_lda = LogisticRegression(max_iter=1000,C = 1.)\n",
    "classifier_lda.fit(X_train_lda, y_train)\n",
    "\n",
    "# Evaluate the classifier\n",
    "y_pred_lda = classifier_lda.predict(X_test_lda)\n",
    "train_accuracy_lda = accuracy_score(y_test, y_pred_lda)\n",
    "\n",
    "print(f\"Training accuracy with LDA embeddings: {train_accuracy_lda:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Entropy Loss for Word2Vec model: 0.4034\n",
      "Cross-Entropy Loss for LSA model: 0.3757\n",
      "Cross-Entropy Loss for LDA model: 0.4329\n"
     ]
    }
   ],
   "source": [
    "# Let us compute the cross-entropy losses for the three models, which is better \n",
    "#suited to classification \n",
    "\n",
    "# Word2Vec\n",
    "y_test_prob_word2vec = classifier.predict_proba(X_test_word2vec)[:, 1]\n",
    "cross_entropy_loss_word2vec = log_loss(y_test, y_test_prob_word2vec)\n",
    "print(f\"Cross-Entropy Loss for Word2Vec model: {cross_entropy_loss_word2vec:.4f}\")\n",
    "\n",
    "# LSA\n",
    "y_test_prob_lsa = classifier_lsa.predict_proba(X_test_lsa)[:, 1]\n",
    "cross_entropy_loss_lsa = log_loss(y_test, y_test_prob_lsa)\n",
    "print(f\"Cross-Entropy Loss for LSA model: {cross_entropy_loss_lsa:.4f}\")\n",
    "\n",
    "# LDA \n",
    "y_test_prob_lda = classifier_lda.predict_proba(X_test_lda)[:, 1]\n",
    "cross_entropy_loss_lda = log_loss(y_test, y_test_prob_lda)\n",
    "print(f\"Cross-Entropy Loss for LDA model: {cross_entropy_loss_lda:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for Word2Vec: 0.8240402564726889\n",
      "Recall for Word2Vec: 0.81224\n",
      "F1-Score for Word2Vec: 0.8180975786632287\n",
      "Confusion Matrix for Word2Vec:\n",
      " [[10332  2168]\n",
      " [ 2347 10153]]\n",
      "\n",
      "Precision for LSA: 0.8361967521769829\n",
      "Recall for LSA: 0.85272\n",
      "F1-Score for LSA: 0.8443775498078979\n",
      "Confusion Matrix for LSA:\n",
      " [[10412  2088]\n",
      " [ 1841 10659]]\n",
      "\n",
      "Precision for LDA: 0.8024720516454101\n",
      "Recall for LDA: 0.81544\n",
      "F1-Score for LDA: 0.8089040552337116\n",
      "Confusion Matrix for LDA:\n",
      " [[ 9991  2509]\n",
      " [ 2307 10193]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Word2Vec\n",
    "y_pred_word2vec = classifier.predict(X_test_word2vec)\n",
    "print(f\"Precision for Word2Vec: {precision_score(y_test, y_pred_word2vec)}\")\n",
    "print(f\"Recall for Word2Vec: {recall_score(y_test, y_pred_word2vec)}\")\n",
    "print(f\"F1-Score for Word2Vec: {f1_score(y_test, y_pred_word2vec)}\")\n",
    "print(f\"Confusion Matrix for Word2Vec:\\n {confusion_matrix(y_test, y_pred_word2vec)}\\n\")\n",
    "\n",
    "# LSA\n",
    "y_pred_lsa = classifier_lsa.predict(X_test_lsa)\n",
    "print(f\"Precision for LSA: {precision_score(y_test, y_pred_lsa)}\")\n",
    "print(f\"Recall for LSA: {recall_score(y_test, y_pred_lsa)}\")\n",
    "print(f\"F1-Score for LSA: {f1_score(y_test, y_pred_lsa)}\")\n",
    "print(f\"Confusion Matrix for LSA:\\n {confusion_matrix(y_test, y_pred_lsa)}\\n\")\n",
    "\n",
    "#LDA\n",
    "y_pred_lda = classifier_lda.predict(X_test_lda)\n",
    "print(f\"Precision for LDA: {precision_score(y_test, y_pred_lda)}\")\n",
    "print(f\"Recall for LDA: {recall_score(y_test, y_pred_lda)}\")\n",
    "print(f\"F1-Score for LDA: {f1_score(y_test, y_pred_lda)}\")\n",
    "print(f\"Confusion Matrix for LDA:\\n {confusion_matrix(y_test, y_pred_lda)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Word2Vec model seems to be the best one (since trained on real reviews to get the embeddings and not only on probabilities distributions that might not be enough given the size of the data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Sentiment classifier** \n",
    "\n",
    "Now, let us train a classifier (logistic regression, SVM or NN) that takes the sentence-level embeddings as input and outputs a binary sentiment prediction. The classifier’s objective is to minimize the binary cross-entropy loss (for classification) and potentially a regularization term that prevents overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Fine-tuning Word Vectors**\n",
    "\n",
    "- During the training process, backpropagate the error from the sentiment classifier through the word embeddings to fine-tune them.\n",
    "- This way, the word embeddings are updated not only based on word co-occurrence (from unsupervised training) but also based on their effectiveness in predicting sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Vector Construction\n",
    "\n",
    "Each document has a latent vector d and each word a latent vector w. In the paper, they model P(w|d) the probability of a word given the document. After training, theu use the inferred document vector d for classification (which is not a sum or average of word embeddings). For inference, they use Bayesian inference (MAP estimation) to infer these document vectors. \n",
    "\n",
    "2 options : \n",
    "- Replication using Bayesian unsupervised model \n",
    "- Use a modern equivalent like Doc2Vec (DM mode) : it trains document embeddings just like the paper but more easily "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare embeddings for model  \n",
    "\n",
    "def prepare_embeddings(data, word2vec_model, max_sequence_length=100, embedding_dim=50, batch_size=1000):\n",
    "    \"\"\"\n",
    "    Convert the tokenized sentences into word embeddings.\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    \n",
    "    # Process in batches to avoid memory overload\n",
    "    for start_idx in range(0, len(data), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(data))\n",
    "        batch = data[start_idx:end_idx]\n",
    "        \n",
    "        batch_embeddings = []\n",
    "        \n",
    "        for sentence in batch:\n",
    "            sentence_embedding = []\n",
    "            \n",
    "            for token_id in sentence['input_ids']:\n",
    "                # Check if the token exists in the Word2Vec model\n",
    "                if token_id in word2vec_model.wv.key_to_index:\n",
    "                    sentence_embedding.append(word2vec_model.wv[token_id])\n",
    "                else:\n",
    "                    sentence_embedding.append(np.zeros(embedding_dim))  # OOV token (zero vector)\n",
    "            \n",
    "            # Ensure sequence length matches max_sequence_length\n",
    "            padding_length = max_sequence_length - len(sentence_embedding)\n",
    "            if padding_length > 0:\n",
    "                sentence_embedding.extend([np.zeros(embedding_dim)] * padding_length)\n",
    "            else:\n",
    "                sentence_embedding = sentence_embedding[:max_sequence_length]\n",
    "            \n",
    "            batch_embeddings.append(np.array(sentence_embedding))\n",
    "        \n",
    "        embeddings.extend(batch_embeddings)\n",
    "    \n",
    "    return np.array(embeddings)\n",
    "\n",
    "\n",
    "train_embedded = prepare_embeddings(padded_train_data, model)\n",
    "test_embedded = prepare_embeddings(padded_test_data, model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Preprocessing\n",
    "   ⤷ Tokenize reviews\n",
    "   ⤷ Build vocab\n",
    "\n",
    "2. Word Vector Initialization\n",
    "   ⤷ Random or GloVe init\n",
    "   ⤷ Embeddings will be fine-tuned via supervision\n",
    "\n",
    "3. Context Window or Full Review\n",
    "   ⤷ Either train on word windows (as in the paper)\n",
    "   ⤷ Or process whole review as a sequence\n",
    "\n",
    "4. Classifier\n",
    "   ⤷ Logistic regression (window-level)\n",
    "   ⤷ OR CNN/LSTM over full review\n",
    "\n",
    "5. Training\n",
    "   ⤷ Cross-entropy loss\n",
    "   ⤷ SGD or Adam\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
